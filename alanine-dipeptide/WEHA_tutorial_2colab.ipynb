{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d318ef12-a24f-453a-9f7d-9692ffbd92e8",
   "metadata": {},
   "source": [
    "# Conformational sampling of Alanine Dipeptide with weighted ensemble Hamiltonian annealing part 2\n",
    "Authors: Baily Ford<br>\n",
    "Email:&nbsp;&nbsp; bwf15@pitt.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3c416c-b206-47e4-ac10-a38bf6b29ee8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e7b307-c07c-463f-adf1-33b28807f4da",
   "metadata": {},
   "source": [
    "Welcome! This notebook is made with the assumption that you're working on `colab.research.google.com` Follow the steps in \"[Setting up the virtual environment](#Setting-up-the-virtual-environment)\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25bb9d3-5d87-437d-81a6-c082767a8e46",
   "metadata": {},
   "source": [
    "### Setting up the virtual environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906a080d-efcd-45e7-bb8f-d2c054a96526",
   "metadata": {},
   "source": [
    "1. Go to https://colab.research.google.com\n",
    "2. Sign in with your Pitt email/password by clicking `sign in` on the top right\n",
    "3. In the `Open notebook` window, click `GitHub`\n",
    "4. Type `bailyford/drug-design2025_weha`\n",
    "5. Select `alanine-dipeptide/WEHA_tutorial_2colab.ipynb`\n",
    "6. Run the following cells. The 4th one will take 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f02b58-196c-4b3e-b599-4e497fc6f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS ONCE. After kernel restart, proceed to the next cell.\n",
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install()\n",
    "\n",
    "## KERNEL WILL RESTART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331dc491-3101-4402-bdeb-bf0a633543cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After kernel restart, start from here:\n",
    "import condacolab\n",
    "condacolab.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d0c5d-ef4d-4566-9738-8302e5d26703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99f0e62-93db-47bf-ba07-fd208665ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --quiet https://github.com/bailyford/drug-design2025_weha\n",
    "%cd drug-design2025_weha/alanine-dipeptide\n",
    "!mamba env update -n base -f env.yaml --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbfcb09-c34d-4161-a49c-63f733145141",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- clustering conformations with Kmeans NANI\n",
    "- use <code>WEDAP</code> to generate Ramanchandra plots  \n",
    "- Visualize the results of clustering with <code>matplotlib</code>\n",
    "- calculate state populations using structure weights "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446afde1-b8bd-4480-bdc4-1e3cf670d153",
   "metadata": {},
   "source": [
    "System Requirements AmberTools24 is necessary to run this simulation. Note that it is already installed as a module on H2P numpy, matplotlib, ipympyl, mdtraj, and nglview are used in this jupyter notebook. nglview, matplotlib, ipympl, and numpy are optional for visualization purposes. These are already installed as part of the virtual environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5414728-80a6-406b-8feb-b9a1dd79a0bb",
   "metadata": {},
   "source": [
    "## TABLE OF CONTENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cab78d8-3689-4f9c-b60b-a8387efd0e4c",
   "metadata": {},
   "source": [
    "[Introduction](#Introduction)\n",
    "\n",
    "Day 2:\n",
    "\n",
    "[4. Visualizing the Results](#4.-Visualizing-the-results)\n",
    "\n",
    "[5. Analyze the MD Results](#5.-Analyze-the-MD-results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee17297-5f4e-414c-83bf-b4f92091b015",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e089c504-5f6f-46b4-86a8-1cacd9ecd6cb",
   "metadata": {},
   "source": [
    "This tutorial is designed to provide an introduction to conformational sampling of biomolecules with the weighted ensemble Hamiltonian Annealing (WEHA) method of the WESTPA software package. It is designed for alanine dipeptide in AMBER 24. This notebook is designed with the assumption that you are working within a virtual environment on the H2P Cluster at Pitt.\n",
    "\n",
    "This is the second part of the tutorial. This will focus on analyzing the simulation with WEDAP, a python package designed for plotting the results of weighted ensemble simulations and cluster data with k-means NANI. K-means NANI improves upon the Kmeans clustering algorithm by deterministically calculating medoids that optimize cluster diversity. The yeilds faster, deterministic clustering that better reflects grouped data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c006bd42-ad5f-4fd2-ac49-4a6daf3a502b",
   "metadata": {},
   "source": [
    "### Preparing supplemental data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17319649-0533-45b3-8bff-408ef7038f71",
   "metadata": {},
   "source": [
    "Only run the following code if you wish to use the supplemental data. This is if you do not have results from part 1. **If you have results you want to use, DO NOT RUN THIS BLOCK.** This will replace your data. As a safety precaution, use_data must be set to True for the code to execute properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb422b-d8aa-422f-b280-5178d4561269",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_data=True\n",
    "if use_data:\n",
    "    !rm -r traj_segs common_files/lambda.dat west.h5\n",
    "    !mv sample_files/west.h5 .\n",
    "    !mv sample_files/lambda.dat common_files/lambda.dat\n",
    "    !mv sample_files/traj_segs ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4229196-8af6-4f00-94d9-f95ed624ed3d",
   "metadata": {},
   "source": [
    "## 4. Visualizing the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b4844f-34ae-47d4-9836-cbe5b111d724",
   "metadata": {},
   "source": [
    "While not required for analysis, it is always recommend to view the file lambda.dat located in the common_files directory. This files tracks the iteration, lambda value, and effective sample size for each iteration and updates as the simulation progresses. Print this file and make sure nothing looks off about the simulation. This could include odd lambda jumps, inconsistent ESS values, or lambda values that do not converge to 1. Close to 1 is fine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec3e76d-c04e-4545-bd1f-08256720a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat common_files/lambda.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8467af23-70e8-48c9-8cd3-f6cfb46cd025",
   "metadata": {},
   "source": [
    "### Analysis with WEDAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3499d09b-1880-41fc-bdc6-878c4c762a7d",
   "metadata": {},
   "source": [
    "WEDAP is a useful tool for plotting the results of weighted ensemble or conventional md simulations. To use WEDAP, we must suply it with our west.h5 file that contains all of our system information. Here, we would like to view the average probability distribution of both our dihedral angles. The resulting plot is similar to a Ramachandran plot. To generate this plot, run the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e336c57-7c8f-408d-b5ac-363dd017ea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wedap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "wedap.H5_Plot(h5='west.h5',data_type = \"average\",first_iter = 2,last_iter=45,plot_mode = 'hist', Yname='psi').plot()\n",
    "plt.xlabel(\"Phi (°)\")\n",
    "plt.ylabel(\"Psi (°)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e958331e-d0bf-4748-bbb5-9d14adcfef3a",
   "metadata": {},
   "source": [
    "While this gives a general overview, all states are dominated by the highest weights to ever visit that state. To make the plot more interperable, we can impose weight thresholds that restrict which weights we see. In the script below, we add a maximum weight of 10 KT. This should make it easier to differentiate unique states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebcec96-562a-491d-acec-50e33f44d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wedap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "wedap.H5_Plot(h5='west.h5',data_type = \"average\",first_iter = 1,last_iter=45,plot_mode = 'hist', Yname='psi', p_max=5).plot()\n",
    "plt.xlabel(\"Phi (°)\")\n",
    "plt.ylabel(\"Psi (°)\")\n",
    "plt.savefig(\"Ramanchandran.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e85bad-e16e-479d-b5d9-0c73f6df0eb5",
   "metadata": {},
   "source": [
    "Additionally, WEDAP is capable of generating GIFs that show how the probability distribution changes over the course of the simulation. This can be useful in finding when certain states are found or die off, and can help visualize transition pathways. Try making a GIF of your simulation data with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e15d75-a75a-4606-97ff-93dd5d093e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "import wedap\n",
    "plot_options = {\"h5\" : \"west.h5\",\n",
    "                \"Xname\" : \"pcoord\",\n",
    "                \"Yname\" : \"psi\",\n",
    "                \"data_type\" : \"average\",\n",
    "                \"p_max\" : 10,\n",
    "                \"p_units\" : \"kcal\",\n",
    "                \"first_iter\" : 1,\n",
    "                \"last_iter\" : 45,\n",
    "                \"plot_mode\" : \"hist\",\n",
    "                \"ylabel\" : \"psi (°)\",\n",
    "                \"xlabel\" : \"phi\",\n",
    "                \"xlim\" : (-180, 180),\n",
    "                \"ylim\" : (-180, 180),\n",
    "                \"grid\" : True,\n",
    "                \"cmap\" : \"gnuplot_r\",\n",
    "                }\n",
    "wedap.make_gif(**plot_options, avg_plus=2, gif_out=\"example.gif\")\n",
    "\n",
    "display(Image(filename=\"example.gif\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7951a93-fd88-41a6-8419-460e33f6d963",
   "metadata": {},
   "source": [
    "## 5. Analyze the MD Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a609d545-fa84-4c64-8233-7ba67df43f18",
   "metadata": {},
   "source": [
    "### Exploring the H5 file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8b463e-5a31-4fa0-8e32-e880f158a3da",
   "metadata": {},
   "source": [
    "WESTPA saves all relevant simulation data in a h5py (.h5) file specified in the west.cfg file. Generally, this files is named west.h5 can in practice have any name. The h5 file has various keys that store a wide variety of data, but for WEHA, the only key we truly care about is the 'iterations' key. This key contains information on our auxillary data, pcoord information, and weigths (seg_index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04c138db-97f0-4ad1-b54a-3f66fa1a359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "with h5py.File('west.h5') as f:\n",
    "    print(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "838cf3b2-fa20-4665-88c7-073ac6f4fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "with h5py.File('west.h5') as f:\n",
    "    print(f['iterations/iter_00000001'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b6852b-3281-4b69-9e09-68040cbeddd2",
   "metadata": {},
   "source": [
    "For our analysis, we need our phi and psi angles as well as the weight of each trajectory. This can be obtained by looping over all iterations and appending the values to a list that we can use later. While we aren't going to use the weights yet, we will be saving them while we are here. The dihedral angles will be saved as dihedral_angles.npy and the weight in weight.dat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ea791f-bcc5-4a08-a622-c1644fe3308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "with h5py.File('west.h5') as f:\n",
    "    phi = []\n",
    "    psi = []\n",
    "    weight = []\n",
    "    normalized_weight = []\n",
    "    for i in f['iterations']:\n",
    "        PHI = f[f'iterations/{i}/pcoord'][:, -1]\n",
    "        PSI = f[f'iterations/{i}/auxdata']['psi'][:, -1]\n",
    "        prob = f[f'iterations/{i}/seg_index']['weight', :]\n",
    "        phi.extend(PHI)\n",
    "        psi.extend(PSI)\n",
    "        weight.extend(prob)\n",
    "for x in weight:\n",
    "    norm_weight = x/sum(weight)\n",
    "    normalized_weight.append(norm_weight)\n",
    "phi = np.array(phi)\n",
    "psi = np.array(psi)\n",
    "normalized_weight = np.array(normalized_weight)\n",
    "combined = np.column_stack((phi, psi)) \n",
    "np.save('dihedral_angles', combined)\n",
    "np.savetxt('weight.dat', normalized_weight)\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5da09d-feee-4335-98dd-be9a80605ff0",
   "metadata": {},
   "source": [
    "Now that we have our data, we need to determine how many conformations we have. We can do this by clustering all states with Kmeans NANI. First we need to determine the optimal number to cluster on. We can do this by clustering over a range of clusters and calculating a DBI score. The DBI score is a metric that measures how unique clusters are. The ideal cluster number will be the value that minimized the DBI score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dadd32b-f3e4-4b4d-be87-d82c2327c87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from mdance.cluster.nani import KmeansNANI, compute_scores\n",
    "from mdance import data\n",
    "from mdance.tools.bts import extended_comparison\n",
    "\n",
    "\n",
    "# System info\n",
    "input_traj_numpy = 'dihedral_angles.npy'\n",
    "N_atoms = 1\n",
    "sieve = 1\n",
    "\n",
    "# NANI parameters\n",
    "output_dir = 'outputs'                        \n",
    "init_types = ['comp_sim']                                           # Must be a list\n",
    "metric = 'MSD'\n",
    "start_n_clusters = 2                                               # At least 2 clusters\n",
    "end_n_clusters = 30                                            # Maximum number of clusters\n",
    "percentage = 100\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    traj_numpy = np.load(input_traj_numpy, allow_pickle=True)[::sieve]\n",
    "    for init_type in init_types:\n",
    "        if init_type in ['k-means++', 'random', 'vanilla_kmeans++']:\n",
    "            percentage = ''\n",
    "        \n",
    "        # `comp_sim` and `div_select` are ran only once to get the initiators\n",
    "        elif init_type in ['comp_sim', 'div_select']:\n",
    "            percentage = 100\n",
    "            mod = KmeansNANI(data=traj_numpy, n_clusters=end_n_clusters, metric=metric, \n",
    "                             N_atoms=N_atoms, init_type=init_type, percentage=percentage)\n",
    "            initiators = mod.initiate_kmeans()\n",
    "        \n",
    "        all_scores = []\n",
    "        for i in trange(start_n_clusters, end_n_clusters+1, desc='cluster'):\n",
    "            total = 0\n",
    "\n",
    "            # Run k-means clustering\n",
    "            if init_type in ['comp_sim', 'div_select']:\n",
    "                mod = KmeansNANI(data=traj_numpy, n_clusters=i, metric=metric, \n",
    "                                 N_atoms=N_atoms, init_type=init_type, percentage=percentage)\n",
    "                labels, centers, n_iter = mod.kmeans_clustering(initiators)\n",
    "            elif init_type in ['k-means++', 'random']:\n",
    "                mod = KmeansNANI(data=traj_numpy, n_clusters=i, metric=metric, \n",
    "                                 N_atoms=N_atoms, init_type=init_type)\n",
    "                labels, centers, n_iter = mod.kmeans_clustering(initiators=init_type)\n",
    "            elif init_type == 'vanilla_kmeans++':\n",
    "                mod = KmeansNANI(data=traj_numpy, n_clusters=i, metric=metric, \n",
    "                                 N_atoms=N_atoms, init_type=init_type)\n",
    "                initiators = mod.initiate_kmeans()\n",
    "                labels, centers, n_iter = mod.kmeans_clustering(initiators=initiators)\n",
    "            \n",
    "            \n",
    "            # Compute scores\n",
    "            ch_score, db_score = compute_scores(data=traj_numpy, labels=labels)\n",
    "            \n",
    "            # Calculate MSD for each cluster\n",
    "            dict = {}\n",
    "            for j in trange(i, desc=f'cluster {i}', leave=False):\n",
    "                dict[j] = np.where(labels == j)[0]\n",
    "                dict[j] = traj_numpy[dict[j]]\n",
    "            for key in dict:\n",
    "                msd = extended_comparison(np.array(dict[key]), traj_numpy_type='full', \n",
    "                                          metric=metric, N_atoms=N_atoms)\n",
    "                total += msd\n",
    "            all_scores.append((i, n_iter, ch_score, db_score, total/i))\n",
    "        \n",
    "        all_scores = np.array(all_scores)\n",
    "        header = f'init_type: {init_type}, percentage: {percentage}, metric: {metric}, sieve: {sieve}\\n'\n",
    "        header += 'Number of clusters, Number of iterations, Calinski-Harabasz score, Davies-Bouldin score, Average MSD'\n",
    "        np.savetxt(f'{output_dir}/{percentage}{init_type}_summary.csv', all_scores, \n",
    "                   delimiter=',', header=header, fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b35507-4b69-4d26-94a4-7103992e5564",
   "metadata": {},
   "source": [
    "The code below will plot the DBI scores and indicates the best values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea39ee2-b2c4-4b3d-af5b-522b1809dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cluster Analysis\n",
    "===============================================\n",
    "\n",
    "MDANCE provides a pipeline to screen the optimal number of clusters for a given dataset.\n",
    "\n",
    "The pwd of this script is ``$PATH/MDANCE/examples``.\n",
    "\"\"\"\n",
    "###############################################################################\n",
    "# To begin with, let's first import the modules we will use:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "###############################################################################\n",
    "# ``scores_csvs`` is the list of screening csv that were outputted from `screen_nani.py`.\n",
    "# The output of this notebook will also be the same directory as the input csvs.\n",
    "\n",
    "scores_csvs = ['outputs/100comp_sim_summary.csv']\n",
    "\n",
    "###############################################################################\n",
    "# The function below will plot the Davies-Bouldin index and the optimal number of clusters.\n",
    "#   - The optimal number of clusters is determined by the minimum Davies-Bouldin index or the minimum of the second derivative of the Davies-Bouldin index.\n",
    "#   - Potential Errors\n",
    "#       - Please remember to remove the row with ``None,None`` in the screening csv if there is an error.\n",
    "\n",
    "def plot_scores(scores_csv):\n",
    "    \"\"\"Plot the Davies-Bouldin index and the optimal number of clusters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    scores_csv : str\n",
    "        The path to the csv file that contains the screening\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    file\n",
    "        A png file that contains the plot of the Davies-Bouldin index and the optimal number of clusters\n",
    "    \"\"\"\n",
    "    base_name = scores_csv.split('\\\\')[-1].split('.csv')[0]\n",
    "    n_clus, db = np.loadtxt(scores_csv, unpack=True, delimiter=',', usecols=(0, 3))\n",
    "\n",
    "    # Plot the Davies-Bouldin index and the optimal number of clusters\n",
    "    all_indices = np.argsort(db)\n",
    "    min_db_index = all_indices[0]\n",
    "    min_db = n_clus[min_db_index]\n",
    "    all_indices = np.delete(all_indices, 0)\n",
    "    second_min_index = all_indices[0]\n",
    "    second_min_db = n_clus[second_min_index]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(n_clus, db, color='#005cde', label='DBI', linewidth=2)\n",
    "    ax.set_xlabel('Cluster Number')\n",
    "    ax.set_ylabel('Davies-Bouldin Index')\n",
    "    ax.axvline(x=min_db, color='#de005c', linestyle='--', label=f'Optimal Cluster Number: {int(min_db)}', linewidth=2)\n",
    "    ax.axvline(x=second_min_db, color='#00ab64', linestyle='--', label=f'Second Optimal Cluster Number: {int(second_min_db)}', linewidth=2)\n",
    "\n",
    "    # Calculate the second derivative (before + after - 2*current)\n",
    "    arr = db\n",
    "    x = n_clus[1:-1]\n",
    "    result = []\n",
    "    for start_index, n_clusters in zip(range(1, len(arr) - 1), x):\n",
    "        temp = arr[start_index + 1] + arr[start_index - 1] - (2 * arr[start_index])\n",
    "        if arr[start_index] <= arr[start_index - 1] and arr[start_index] <= arr[start_index + 1]:\n",
    "            result.append((n_clusters, temp))\n",
    "    result = np.array(result)\n",
    "    if len(result) == 0:\n",
    "        print('No maxima found')\n",
    "    elif len(result) >= 1:\n",
    "        sorted_indices = np.argsort(result[:, 1])[::-1]\n",
    "        sorted_result = result[sorted_indices]\n",
    "        min_x = sorted_result[0][0]\n",
    "        ax.axvline(x=min_x, color='#de8200', linestyle='--', label=f'Optimal 2nd deriv Cluster Number: {int(min_x)}', linewidth=2)\n",
    "        if len(sorted_result) >= 2:\n",
    "            sec_min_x = sorted_result[1][0]\n",
    "            ax.axvline(x=sec_min_x, color='#6400ab', linestyle='--', label=f'Second Optimal 2nd deriv Cluster Number: {int(sec_min_x)}', linewidth=2)\n",
    "    \n",
    "    ax.legend(fontsize=10)\n",
    "    plt.show()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    for scores_csv in scores_csvs:\n",
    "        plot_scores(scores_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80478cb-f748-43ec-9fda-20f60691ecda",
   "metadata": {},
   "source": [
    "Now that we have the optimal cluster number, we need to assign each trajectory to a cluster. We can initiate Kmeans NANI with the code below. Be sure to set n_clusters equal to value you get if you use your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3e7da4-d6dc-4897-aea9-a296f9888614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mdance.cluster.nani import KmeansNANI\n",
    "from mdance import data\n",
    "from mdance.tools.bts import extended_comparison, calculate_medoid\n",
    "\n",
    "\n",
    "# System info - EDIT THESE\n",
    "input_traj_numpy = 'dihedral_angles.npy'\n",
    "N_atoms = 1\n",
    "sieve = 1\n",
    "\n",
    "# K-means params - EDIT THESE\n",
    "n_clusters = 6\n",
    "init_type = 'comp_sim'                                              # Default\n",
    "metric = 'MSD'                                                      # Default\n",
    "n_structures = 1                                                   # Default\n",
    "output_dir = 'outputs'                                              # Default\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    traj_numpy = np.load(input_traj_numpy)[::sieve]\n",
    "    mod = KmeansNANI(data=traj_numpy, n_clusters=n_clusters, N_atoms=N_atoms, init_type=init_type, \n",
    "                     metric=metric, percentage=10)\n",
    "    labels, centers, n_iter = mod.execute_kmeans_all()\n",
    "    sort_labels_by_size = np.argsort(np.bincount(labels))[::-1]\n",
    "    labels = np.array([np.where(sort_labels_by_size == i)[0][0] for i in labels])\n",
    "    best_frames = []\n",
    "    cluster_msd = []\n",
    "\n",
    "    # Save best frames indices for each cluster\n",
    "    for i, label in enumerate(np.unique(labels)):\n",
    "        cluster = np.where(labels == label)[0]\n",
    "        if len(cluster) > 1:\n",
    "            medoid_index = calculate_medoid(traj_numpy[cluster], metric=metric, N_atoms=N_atoms)\n",
    "            medoid = traj_numpy[cluster][medoid_index]\n",
    "            msd_to_medoid = []\n",
    "            for j, frame in enumerate(traj_numpy[cluster]):\n",
    "                msd_to_medoid.append((j, extended_comparison(\n",
    "                    np.array([frame, medoid]), data_type='full', metric=metric, N_atoms=N_atoms)))\n",
    "            msd_to_medoid = np.array(msd_to_medoid)\n",
    "            sorted_indices = np.argsort(msd_to_medoid[:, 1])\n",
    "            best_n_structures = traj_numpy[cluster][sorted_indices[:n_structures]]\n",
    "            best_frames.append(best_n_structures)\n",
    "    \n",
    "    best_frames_indices = []\n",
    "    for i, frame in enumerate(traj_numpy):\n",
    "        i = i * sieve\n",
    "        for j, cluster in enumerate(best_frames):\n",
    "            if np.any(np.all(cluster == frame, axis=1)):\n",
    "                best_frames_indices.append((i, j))\n",
    "    best_frames_indices = np.array(best_frames_indices)\n",
    "    best_frames_indices = best_frames_indices[best_frames_indices[:, 1].argsort()]\n",
    "    np.savetxt(f'{output_dir}/best_frames_indices_{n_clusters}.csv', best_frames_indices, delimiter=',', fmt='%s', \n",
    "               header=f'Numer of clusters,{n_clusters}\\nFrame Index,Cluster Index')\n",
    "    \n",
    "    # Save cluster labels\n",
    "    with open(f'{output_dir}/labels_{n_clusters}.csv', 'w') as f:\n",
    "        f.write(f'# init_type: {init_type}, Number of clusters: {n_clusters}\\n')\n",
    "        f.write('# Frame Index, Cluster Index\\n')\n",
    "        for i, row in enumerate(labels):\n",
    "            f.write(f'{i * sieve},{row}\\n')\n",
    "    \n",
    "    # Calculate population of each cluster\n",
    "    with open(f'{output_dir}/summary_{n_clusters}.csv', 'w') as f:\n",
    "        f.write(f'# Number of clusters, {n_clusters}\\n')\n",
    "        f.write('# Cluster Index, Fraction out of total pixels\\n')\n",
    "        for i, row in enumerate(np.bincount(labels)):\n",
    "            f.write(f'{i},{row/len(labels)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04e7435-b4fd-4c15-ac0d-d6b8687f4e33",
   "metadata": {},
   "source": [
    "NANI will output the reulting files in a new directory named outputs. There are 3 files: labels.csv which contain each frame and its corresponding cluster, best_frames_indices which indicates the frame used as the medoid of each clsuster, and summary.csv that gives an approximate population of each cluster. Using the labels file, we can plot our dihedral angles to see how well the clustering performed. To do so, run the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b875e4b-856f-45e3-b373-5b64d9248f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "\n",
    "label_file = 'outputs/labels_6.csv'\n",
    "df = pd.read_csv(label_file, header=1)\n",
    "angles = 'dihedral_angles.npy'\n",
    "dihedrals = np.load(angles)\n",
    "df = pd.concat([df, pd.DataFrame(dihedrals)], axis=1)\n",
    "df.to_csv('outputs/combined_data.csv', index=False)\n",
    "# Load your data\n",
    "file_path = \"outputs/combined_data.csv\"  # Change this to your actual file path\n",
    "df = pd.read_csv(file_path, delimiter=',', header=0, names=[\"index\", \"cluster_label\", \"phi\", \"psi\"])\n",
    "\n",
    "# Extract relevant columns\n",
    "phi = df[\"phi\"]\n",
    "psi = df[\"psi\"]\n",
    "clusters = df[\"cluster_label\"]\n",
    "\n",
    "# Generate distinct colors using Matplotlib's 'tab10' colormap\n",
    "unique_clusters = np.unique(clusters)\n",
    "palette = plt.get_cmap(\"tab10\")  # 'tab10' gives 10 distinct colors\n",
    "\n",
    "# Assign colors\n",
    "cluster_colors = {cluster: palette(i % 10) for i, cluster in enumerate(unique_clusters)}\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "for cluster in unique_clusters:\n",
    "    mask = clusters == cluster\n",
    "    plt.scatter(phi[mask], psi[mask], s=20, color=cluster_colors[cluster], edgecolor=\"black\", linewidth=0.5, alpha=0.7)\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel(\"Phi (°)\")\n",
    "plt.ylabel(\"Psi (°)\")\n",
    "plt.title(\"Clustered Ramachandran Plot\")\n",
    "plt.xlim(-180, 180)\n",
    "plt.ylim(-180, 180)\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Create legend patches manually\n",
    "legend_patches = [mpatches.Patch(color=cluster_colors[cluster], label=f\"Cluster {cluster}\") for cluster in unique_clusters]\n",
    "\n",
    "plt.legend(handles=legend_patches, title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1106ada5-86ff-4695-8325-2a47ce64c964",
   "metadata": {},
   "source": [
    "You may notice that the clusters that cross the periodic boundary get classified as their own cluster. This is obviously not ideal. To combat this, we can remove the periodic boundaries by taking the sine and cosine values of each angle. This basically converst us from a 2d plane to a sphere. We can cluster the sphericle data, then replot it. Unfortunately, this does mean that we should recluster our data and repeat the steps from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba56ae-7d06-42e1-be0d-953d591d8050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "file_path = np.load(\"dihedral_angles.npy\")\n",
    "df = pd.DataFrame(file_path, columns=['phi', 'psi'])\n",
    "\n",
    "# Extract phi/psi angles\n",
    "phi_rad = np.deg2rad(df[\"phi\"])\n",
    "psi_rad = np.deg2rad(df[\"psi\"])\n",
    "\n",
    "# Convert to circular space\n",
    "df[\"phi_sin\"], df[\"phi_cos\"] = np.sin(phi_rad), np.cos(phi_rad)\n",
    "df[\"psi_sin\"], df[\"psi_cos\"] = np.sin(psi_rad), np.cos(psi_rad)\n",
    "trig_columns = [\"phi_sin\", \"phi_cos\", \"psi_sin\", \"psi_cos\"]\n",
    "trig_array = df[trig_columns].to_numpy()\n",
    "\n",
    "# Save to .npy file\n",
    "np.save(\"trig_data.npy\", trig_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b86507-5191-41ec-b4a4-b010487a279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from mdance.cluster.nani import KmeansNANI, compute_scores\n",
    "from mdance import data\n",
    "from mdance.tools.bts import extended_comparison\n",
    "\n",
    "\n",
    "# System info\n",
    "input_traj_numpy = 'trig_data.npy'\n",
    "N_atoms = 1\n",
    "sieve = 1\n",
    "\n",
    "# NANI parameters\n",
    "output_dir = 'outputs'                        \n",
    "init_types = ['comp_sim']                                           # Must be a list\n",
    "metric = 'MSD'\n",
    "start_n_clusters = 2                                               # At least 2 clusters\n",
    "end_n_clusters = 30                                            # Maximum number of clusters\n",
    "percentage = 100\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    traj_numpy = np.load(input_traj_numpy, allow_pickle=True)[::sieve]\n",
    "    for init_type in init_types:\n",
    "        if init_type in ['k-means++', 'random', 'vanilla_kmeans++']:\n",
    "            percentage = ''\n",
    "        \n",
    "        # `comp_sim` and `div_select` are ran only once to get the initiators\n",
    "        elif init_type in ['comp_sim', 'div_select']:\n",
    "            percentage = 100\n",
    "            mod = KmeansNANI(data=traj_numpy, n_clusters=end_n_clusters, metric=metric, \n",
    "                             N_atoms=N_atoms, init_type=init_type, percentage=percentage)\n",
    "            initiators = mod.initiate_kmeans()\n",
    "        \n",
    "        all_scores = []\n",
    "        for i in trange(start_n_clusters, end_n_clusters+1, desc='cluster'):\n",
    "            total = 0\n",
    "\n",
    "            # Run k-means clustering\n",
    "            if init_type in ['comp_sim', 'div_select']:\n",
    "                mod = KmeansNANI(data=traj_numpy, n_clusters=i, metric=metric, \n",
    "                                 N_atoms=N_atoms, init_type=init_type, percentage=percentage)\n",
    "                labels, centers, n_iter = mod.kmeans_clustering(initiators)\n",
    "            elif init_type in ['k-means++', 'random']:\n",
    "                mod = KmeansNANI(data=traj_numpy, n_clusters=i, metric=metric, \n",
    "                                 N_atoms=N_atoms, init_type=init_type)\n",
    "                labels, centers, n_iter = mod.kmeans_clustering(initiators=init_type)\n",
    "            elif init_type == 'vanilla_kmeans++':\n",
    "                mod = KmeansNANI(data=traj_numpy, n_clusters=i, metric=metric, \n",
    "                                 N_atoms=N_atoms, init_type=init_type)\n",
    "                initiators = mod.initiate_kmeans()\n",
    "                labels, centers, n_iter = mod.kmeans_clustering(initiators=initiators)\n",
    "            \n",
    "            \n",
    "            # Compute scores\n",
    "            ch_score, db_score = compute_scores(data=traj_numpy, labels=labels)\n",
    "            \n",
    "            # Calculate MSD for each cluster\n",
    "            dict = {}\n",
    "            for j in range(i):\n",
    "                dict[j] = np.where(labels == j)[0]\n",
    "                dict[j] = traj_numpy[dict[j]]\n",
    "            for key in dict:\n",
    "                msd = extended_comparison(np.array(dict[key]), traj_numpy_type='full', \n",
    "                                          metric=metric, N_atoms=N_atoms)\n",
    "                total += msd\n",
    "            all_scores.append((i, n_iter, ch_score, db_score, total/i))\n",
    "        \n",
    "        all_scores = np.array(all_scores)\n",
    "        header = f'init_type: {init_type}, percentage: {percentage}, metric: {metric}, sieve: {sieve}\\n'\n",
    "        header += 'Number of clusters, Number of iterations, Calinski-Harabasz score, Davies-Bouldin score, Average MSD'\n",
    "        np.savetxt(f'{output_dir}/{percentage}{init_type}_summary.csv', all_scores, \n",
    "                   delimiter=',', header=header, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b335c57-8680-42eb-ab29-a2e6b6acad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cluster Analysis\n",
    "===============================================\n",
    "\n",
    "MDANCE provides a pipeline to screen the optimal number of clusters for a given dataset.\n",
    "\n",
    "The pwd of this script is ``$PATH/MDANCE/examples``.\n",
    "\"\"\"\n",
    "###############################################################################\n",
    "# To begin with, let's first import the modules we will use:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "###############################################################################\n",
    "# ``scores_csvs`` is the list of screening csv that were outputted from `screen_nani.py`.\n",
    "# The output of this notebook will also be the same directory as the input csvs.\n",
    "\n",
    "scores_csvs = ['outputs/100comp_sim_summary.csv']\n",
    "\n",
    "###############################################################################\n",
    "# The function below will plot the Davies-Bouldin index and the optimal number of clusters.\n",
    "#   - The optimal number of clusters is determined by the minimum Davies-Bouldin index or the minimum of the second derivative of the Davies-Bouldin index.\n",
    "#   - Potential Errors\n",
    "#       - Please remember to remove the row with ``None,None`` in the screening csv if there is an error.\n",
    "\n",
    "def plot_scores(scores_csv):\n",
    "    \"\"\"Plot the Davies-Bouldin index and the optimal number of clusters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    scores_csv : str\n",
    "        The path to the csv file that contains the screening\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    file\n",
    "        A png file that contains the plot of the Davies-Bouldin index and the optimal number of clusters\n",
    "    \"\"\"\n",
    "    base_name = scores_csv.split('\\\\')[-1].split('.csv')[0]\n",
    "    n_clus, db = np.loadtxt(scores_csv, unpack=True, delimiter=',', usecols=(0, 3))\n",
    "\n",
    "    # Plot the Davies-Bouldin index and the optimal number of clusters\n",
    "    all_indices = np.argsort(db)\n",
    "    min_db_index = all_indices[0]\n",
    "    min_db = n_clus[min_db_index]\n",
    "    all_indices = np.delete(all_indices, 0)\n",
    "    second_min_index = all_indices[0]\n",
    "    second_min_db = n_clus[second_min_index]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(n_clus, db, color='#005cde', label='DBI', linewidth=2)\n",
    "    ax.set_xlabel('Cluster Number')\n",
    "    ax.set_ylabel('Davies-Bouldin Index')\n",
    "    ax.axvline(x=min_db, color='#de005c', linestyle='--', label=f'Optimal Cluster Number: {int(min_db)}', linewidth=2)\n",
    "    ax.axvline(x=second_min_db, color='#00ab64', linestyle='--', label=f'Second Optimal Cluster Number: {int(second_min_db)}', linewidth=2)\n",
    "\n",
    "    # Calculate the second derivative (before + after - 2*current)\n",
    "    arr = db\n",
    "    x = n_clus[1:-1]\n",
    "    result = []\n",
    "    for start_index, n_clusters in zip(range(1, len(arr) - 1), x):\n",
    "        temp = arr[start_index + 1] + arr[start_index - 1] - (2 * arr[start_index])\n",
    "        if arr[start_index] <= arr[start_index - 1] and arr[start_index] <= arr[start_index + 1]:\n",
    "            result.append((n_clusters, temp))\n",
    "    result = np.array(result)\n",
    "    if len(result) == 0:\n",
    "        print('No maxima found')\n",
    "    elif len(result) >= 1:\n",
    "        sorted_indices = np.argsort(result[:, 1])[::-1]\n",
    "        sorted_result = result[sorted_indices]\n",
    "        min_x = sorted_result[0][0]\n",
    "        ax.axvline(x=min_x, color='#de8200', linestyle='--', label=f'Optimal 2nd deriv Cluster Number: {int(min_x)}', linewidth=2)\n",
    "        if len(sorted_result) >= 2:\n",
    "            sec_min_x = sorted_result[1][0]\n",
    "            ax.axvline(x=sec_min_x, color='#6400ab', linestyle='--', label=f'Second Optimal 2nd deriv Cluster Number: {int(sec_min_x)}', linewidth=2)\n",
    "    \n",
    "    ax.legend(fontsize=10)\n",
    "    plt.show()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    for scores_csv in scores_csvs:\n",
    "        plot_scores(scores_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f15e10f-fad7-462b-aaaa-778580fa8c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mdance.cluster.nani import KmeansNANI\n",
    "from mdance import data\n",
    "from mdance.tools.bts import extended_comparison, calculate_medoid\n",
    "\n",
    "\n",
    "# System info - EDIT THESE\n",
    "input_traj_numpy = 'trig_data.npy'\n",
    "N_atoms = 1\n",
    "sieve = 1\n",
    "\n",
    "# K-means params - EDIT THESE\n",
    "n_clusters = 8\n",
    "init_type = 'comp_sim'                                              # Default\n",
    "metric = 'MSD'                                                      # Default\n",
    "n_structures = 1                                                   # Default\n",
    "output_dir = 'outputs'                                              # Default\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    traj_numpy = np.load(input_traj_numpy)[::sieve]\n",
    "    mod = KmeansNANI(data=traj_numpy, n_clusters=n_clusters, N_atoms=N_atoms, init_type=init_type, \n",
    "                     metric=metric, percentage=10)\n",
    "    labels, centers, n_iter = mod.execute_kmeans_all()\n",
    "    sort_labels_by_size = np.argsort(np.bincount(labels))[::-1]\n",
    "    labels = np.array([np.where(sort_labels_by_size == i)[0][0] for i in labels])\n",
    "    best_frames = []\n",
    "    cluster_msd = []\n",
    "\n",
    "    # Save best frames indices for each cluster\n",
    "    for i, label in enumerate(np.unique(labels)):\n",
    "        cluster = np.where(labels == label)[0]\n",
    "        if len(cluster) > 1:\n",
    "            medoid_index = calculate_medoid(traj_numpy[cluster], metric=metric, N_atoms=N_atoms)\n",
    "            medoid = traj_numpy[cluster][medoid_index]\n",
    "            msd_to_medoid = []\n",
    "            for j, frame in enumerate(traj_numpy[cluster]):\n",
    "                msd_to_medoid.append((j, extended_comparison(\n",
    "                    np.array([frame, medoid]), data_type='full', metric=metric, N_atoms=N_atoms)))\n",
    "            msd_to_medoid = np.array(msd_to_medoid)\n",
    "            sorted_indices = np.argsort(msd_to_medoid[:, 1])\n",
    "            best_n_structures = traj_numpy[cluster][sorted_indices[:n_structures]]\n",
    "            best_frames.append(best_n_structures)\n",
    "    \n",
    "    best_frames_indices = []\n",
    "    for i, frame in enumerate(traj_numpy):\n",
    "        i = i * sieve\n",
    "        for j, cluster in enumerate(best_frames):\n",
    "            if np.any(np.all(cluster == frame, axis=1)):\n",
    "                best_frames_indices.append((i, j))\n",
    "    best_frames_indices = np.array(best_frames_indices)\n",
    "    best_frames_indices = best_frames_indices[best_frames_indices[:, 1].argsort()]\n",
    "    np.savetxt(f'{output_dir}/best_frames_indices_{n_clusters}.csv', best_frames_indices, delimiter=',', fmt='%s', \n",
    "               header=f'Numer of clusters,{n_clusters}\\nFrame Index,Cluster Index')\n",
    "    \n",
    "    # Save cluster labels\n",
    "    with open(f'{output_dir}/labels_{n_clusters}.csv', 'w') as f:\n",
    "        f.write(f'# init_type: {init_type}, Number of clusters: {n_clusters}\\n')\n",
    "        f.write('# Frame Index, Cluster Index\\n')\n",
    "        for i, row in enumerate(labels):\n",
    "            f.write(f'{i * sieve},{row}\\n')\n",
    "    \n",
    "    # Calculate population of each cluster\n",
    "    with open(f'{output_dir}/summary_{n_clusters}.csv', 'w') as f:\n",
    "        f.write(f'# Number of clusters, {n_clusters}\\n')\n",
    "        f.write('# Cluster Index, Fraction out of total pixels\\n')\n",
    "        for i, row in enumerate(np.bincount(labels)):\n",
    "            f.write(f'{i},{row/len(labels)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd25bbf9-4262-41a2-be44-4131e9411923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 🔹 Load your data\n",
    "labels_file = 'outputs/labels_6.csv'\n",
    "labels_data = np.loadtxt(labels_file, delimiter=',', skiprows=2)\n",
    "\n",
    "# Load the data from output3.txt\n",
    "output3 = np.load('dihedral_angles.npy', allow_pickle=True)\n",
    "\n",
    "# Combine the original labels data with the output3 column\n",
    "combined_data = np.column_stack((labels_data, output3))\n",
    "\n",
    "# Save the new combined data to a CSV file\n",
    "combined_file_path = 'outputs/trig_combined_data.txt'\n",
    "np.savetxt(combined_file_path, combined_data, delimiter=',', header='index,cluster_label,phi,psi', comments='')\n",
    "\n",
    "# 🔹 Load the combined dataset\n",
    "df = pd.read_csv(combined_file_path, delimiter=',', header=0, names=[\"index\", \"cluster_label\", \"phi\", \"psi\"])\n",
    "\n",
    "# 🔹 Extract values\n",
    "phi = df[\"phi\"]\n",
    "psi = df[\"psi\"]\n",
    "clusters = df[\"cluster_label\"]\n",
    "\n",
    "# 🔹 Define a colorblind-friendly palette (Paul Tol's high-contrast colors)\n",
    "tol_colors = [\n",
    "   \"tab:blue\", \"tab:purple\", \"tab:green\", \"black\", \"tab:grey\", \"tab:orange\"\n",
    "]  # Expand if more clusters are needed\n",
    "\n",
    "# Ensure enough colors for the number of unique clusters\n",
    "unique_clusters = np.unique(clusters)\n",
    "num_clusters = len(unique_clusters)\n",
    "\n",
    "# Assign colors to clusters (cycling through if needed)\n",
    "cluster_colors = {cluster: tol_colors[i % len(tol_colors)] for i, cluster in enumerate(unique_clusters)}\n",
    "\n",
    "# 🔹 Create the scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "for cluster in unique_clusters:\n",
    "    mask = clusters == cluster\n",
    "    plt.scatter(phi[mask], psi[mask], s=10, color=cluster_colors[cluster], label=f\"Cluster {int(cluster)}\", alpha=0.7)\n",
    "\n",
    "# 🔹 Format the plot\n",
    "plt.xlabel(\"Phi (°)\")\n",
    "plt.ylabel(\"Psi (°)\")\n",
    "plt.title(\"Clustered Ramachandran Plot \")\n",
    "plt.xlim(-180, 180)\n",
    "plt.ylim(-180, 180)\n",
    "plt.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=10)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# 🔹 Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96318e0d-dd2a-4fec-8bda-b370350ddbcd",
   "metadata": {},
   "source": [
    "Much better. Now we can determine state populations by using each cluster as state and summing all the weight within the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9de6a2-170c-4f8e-998b-19506765c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "weights = pd.read_csv('weight.dat', header=None, names=['weights'])\n",
    "labels = pd.read_csv('outputs/labels_6.csv', delimiter=',', header=1, names=['Frame', 'Cluster'])\n",
    "df = pd.concat([labels, weights], axis=1)\n",
    "df.drop('Frame', axis = 1, inplace=True)\n",
    "cluster_weight_sum = df.groupby('Cluster')['weights'].sum()\n",
    "print(cluster_weight_sum*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fb7929-4c38-42a4-8652-42663a2a7451",
   "metadata": {},
   "source": [
    "Now we can pull out representative structures by using the best_frame_indices values. These are found in 'outputs/best_frames_indices_6.csv'. Reporting in frames is difficult to parse, so a function has been provided that will tell you the path to the corresponding frame. Read the file with the cat function below and use the values in the 'Frame Index' column in the find_frame function below. Be sure to include the number of segments per iteration as well as the total number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251881e1-d521-4159-bf1f-f62a1e1b050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "def find_frame(n_segs, iters, frame):\n",
    "    frame = int(frame)\n",
    "    n_segs = int(n_segs)\n",
    "    iters = int(iters)\n",
    "    total_frames = n_segs * iters\n",
    "    percent = frame/total_frames\n",
    "    decimal, integer = math.modf(percent*iters)\n",
    "    iteration = int(integer + 1)\n",
    "    segment = round(decimal*n_segs)\n",
    "    print(f'Ideal structure in iteration {iteration}, segment {segment}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff4f46d-bd7a-4b21-a105-00594a79dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat outputs/best_frames_indices_6.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a0c13-f273-4721-8556-dfc127558556",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_frame(100, 45, 495)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7ed3ed-4174-4793-a87a-4bf5ba050c15",
   "metadata": {},
   "source": [
    "The structures can be viewed with the code below. Note that we are only concerned with the final frames of each trajectory.\n",
    "\n",
    "NOTE: THE FOLLOWING CELL MIGHT NOT WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6da593-817d-45c5-8dfd-f036017ebec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import mdtraj as md\n",
    "import nglview\n",
    "\n",
    "# Loading in the trajectory\n",
    "# Use the second line to read from sample_files instead.\n",
    "traj_files = ['traj_segs/000005/000095/seg.nc']\n",
    "\n",
    "# Load trajectories\n",
    "a = md.load(traj_files, top='common_files/diala.prmtop')\n",
    "# Passing the MDTraj trajectory to nglview\n",
    "#getting final frame\n",
    "final_frame = a[-1]\n",
    "view = nglview.show_mdtraj(final_frame)\n",
    "\n",
    "# Display both protein and the water\n",
    "view.representations = [\n",
    "    {\"type\": \"ball+stick\", \"params\": {\n",
    "        \"sele\": \"protein\",\n",
    "    }}\n",
    "]\n",
    "\n",
    "# Finally, show us the visualization\n",
    "view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cd6ba4-dee3-46c1-877c-505b759a767b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
